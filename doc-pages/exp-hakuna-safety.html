<div class="doc-case" data-template="exp-hakuna-safety">
    <h3>
        Hakuna Safety
    </h3>
    <div class="doc-case__section">
        <h4>Background</h4>
        <p>
            As Hakuna scaled globally, safety incidents risked regulatory scrutiny and creator churn; moderation combined machine learning classifiers with a distributed reviewer workforce.
        </p>
    </div>
    <div class="doc-case__section">
        <h4>Problem</h4>
        <p>
            Review queues were noisy, label quality varied, and ops leaders lacked visibility into spikesâ€”causing harmful content to slip through while safe streams were mistakenly flagged.
        </p>
    </div>
    <div class="doc-case__section">
        <h4>Solution</h4>
        <p>
            Reworked annotation guidelines, tightened reviewer QA loops, and shipped a real-time safety dashboard exposing classifier confidence, queue health, and escalation SLAs; introduced continuous retraining pipelines.
        </p>
    </div>
    <div class="doc-case__section">
        <h4>Analytics &amp; Iteration</h4>
        <p>
            Monitored precision/recall per violation type, ran side-by-side tests of new models, and embedded proactive alerts for spikes in nudity or harassment signals to trigger human sweeps instantly.
        </p>
    </div>
    <div class="doc-case__section">
        <h4>Result</h4>
        <p>
            Detection accuracy climbed 30%, review latency halved, and appeals dropped by 25%; trust &amp; safety satisfaction scores improved as incident post-mortems became data-driven.
        </p>
    </div>
    <div class="doc-case__section">
        <h4>Follow-up Actions</h4>
        <p>
            Operationalized monthly calibration sessions with reviewers, set up a backlog for emerging policy categories, and aligned with legal on reporting cadence for key markets.
        </p>
    </div>
</div>
